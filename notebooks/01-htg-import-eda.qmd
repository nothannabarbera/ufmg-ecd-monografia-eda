---
title: "EDA: Ingestão e tratamento de dados inicial do dataset"
author: "Hanna Tatsuta Galassi"
date: "2026-01-10"
format: html
toc: true
echo: false
standalone: true
---

```{python}
from datetime import date, timedelta, datetime
import os

import kagglehub
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.stats import gamma, lognorm, weibull_min, poisson, kstest
from scipy.optimize import minimize

```

# Visão geral dos dados

O dataset encontra-se disponível para download na plataforma Kaggle, no link [https://www.kaggle.com/datasets/edumagalhaes/quality-prediction-in-a-mining-process](https://www.kaggle.com/datasets/edumagalhaes/quality-prediction-in-a-mining-process)

Os dados são referentes a um processo de flotação em uma planta de minério de ferro.

**TO-DO**: adicionar breve descrição do processo de flotação

Algumas variáveis são medidas com sensores online, e nesse dataset estão amostradas a cada 20 segundos. Outras - entre elas a concentração de Sílica na corrente de Concentrado, variável de interesse - são medidas em laboratório a cada 2 horas.

Nesse dataset, as variáveis com amostragem a cada 2 horas tiveram os valores vazios previamente preenchidos com o último valor disponível, gerando o comportamento de "congelamento" que será observado nos gráficos temporais. Tal comportamento será devidamente tratado antes das análises.


```{python}
dataset_path = "edumagalhaes/quality-prediction-in-a-mining-process"
download_dir = "../data/raw"
filename = "/MiningProcess_Flotation_Plant_Database.csv"
```

```{python}
os.environ['KAGGLEHUB_CACHE'] = download_dir

download_path = kagglehub.dataset_download(dataset_path)
```

```{python}
path_df = download_path + filename

df_data = pd.read_csv(path_df, decimal=",", parse_dates=["date"])
```

## Visualização dos dados importados

```{python}
df_data.head()
```

## Colunas presentes no dataset

```{python}
for column in df_data.columns:
    print(column)
```

## Tratamento da coluna "date"

A coluna 'date' contém dados no formato "ano-mês-dia hora:minuto:segundo", e os valores variam com incremento de uma hora. Apesar do formato, os minutos e segundos de cada linha não são especificados nessa coluna, e por isso há repetição de valores de data. Para cada 1h de dados, temos 180 pontos (informação corroborada pela descrição no Kaggle, que especifica que há no dataframe variáveis cuja coleta ocorre a cada 20 segundos)

Seria vantajoso para a análise utilizar essa coluna como índice do dataframe. Para isso, é importante tratar a coluna adicionando os minutos e segundos, de maneira a ter valores únicos de data e hora.

```{python}
data_start = str(df_data['date'].min())
data_end = str(df_data['date'].max())

print(f'Data inicial: {data_start}')
print(f'Data final: {data_end}')
```

```{python}
df_data['aux copia do indice'] = df_data.index
df_data['date_fixed'] = df_data.apply(
    lambda x: (x['date'] + (
        ((x['aux copia do indice'] + 4) % 180) * timedelta(seconds = 20))),
        axis = 1)
df_data = df_data.drop(columns = ['aux copia do indice'])
df_data[['date', 'date_fixed']]
```

Uma vez tratada e sem valores repetidos, a coluna 'date_fixed' pode ser utilizada como índice do dataframe

```{python}
df_data = df_data.drop(columns = ['date'])
df_data.set_index('date_fixed', inplace = True)
df_data.sort_index(inplace = True)

df_data.head()
```

# Visualização das variáveis

## Visualização temporal

Para termos uma noção inicial do comportamento temporal das variáveis presentes no dataset, plotamos a série temporal de todas as variáveis.

```{python}
plot_start = data_start #'2017-05-01 00:00:00' #
plot_end = data_end #'2017-05-02 00:00:00' #

variables_to_plot = df_data.columns
```

```{python}
df_plot = df_data
df_plot = df_plot[((df_plot.index > plot_start) & 
                   (df_plot.index < plot_end))]
df_plot = df_plot[variables_to_plot]
```

```{python}
for i in variables_to_plot:
    f, ax = plt.subplots(figsize = (12,6.75))
    plt.title(i)
    yplot = df_plot[i]
    plt.plot(yplot, color = '#023e56')
    plt.show()
    plt.close()
```

1. Os dados aparentam já ter passado por um pré-tratamento. Não parece haver períodos de parada do processo (que poderiam ser identificados por reduções significativas na alimentação de minério de ferro, por exemplo). Além disso, os valores máximos e mínimos da maioria das variáveis estão dentro de um range de (média +- 3 desvios padrões) da própria variável. Isso indica que essa base de dados já passou por algum procedimento de remoção de outliers. Por essa razão, inicialmente não será realizado na presente análise um procedimento de retirada de outliers

2. As variáveis 'Flotation Column 04 Air Flow' e 'Flotation Column 05 Air Flow' parecem estar congeladas no início do período (até 24/04/2017). Caso essas variáveis sejam utilizadas na análise ou na construção do modelo, será necessário remover ou tratar esse período.

3. As variáveis '% Iron Feed' e '% Silica Feed' também aparentam estar congeladas em alguns períodos (períodos diferentes do citado no item 2). Assim como no item 2, caso essas variáveis sejam utilizadas na análise ou na construção do modelo, será necessário remover ou tratar esse período.

## Visualização das distribuições

```{python}
df_plot.describe()
```

```{python}
df_plot.hist(column=variables_to_plot,
             grid=True,
             figsize=(15,45), 
             layout = [9,3],  
             color = '#023e56')
```

A concentração de Silica na corrente de Concentrado ("% Silica Concentrate"), variável de interesse, apresenta uma distribuição assimétrica.

Pelo formato do histograma da distribuição, poderia ser uma Gamma ou uma Lognormal. 

```{python}
def load_and_validate_data(df: pd.DataFrame, column: str) -> np.ndarray:
    """Load and validate positive continuous data from a DataFrame column."""
    if column not in df.columns:
        raise ValueError(f"Column '{column}' not found in DataFrame.")
    data = df[column].dropna().values
    if np.any(data <= 0):
        raise ValueError("Data must be positive for these distributions.")
    return data


def fit_gamma_distribution(data: np.ndarray) -> tuple:
    """Fit Gamma distribution using MLE, compute AIC and KS test."""
    shape, loc, scale = gamma.fit(data, floc=0)
    ll = np.sum(gamma.logpdf(data, shape, loc=loc, scale=scale))
    aic = 2 * 2 - 2 * ll
    ks_stat, ks_p = kstest(data, 'gamma', args=(shape, loc, scale))
    return ("Gamma", shape, scale, aic, ks_stat, ks_p)


def fit_lognormal_distribution(data: np.ndarray) -> tuple:
    """Fit Lognormal distribution using MLE, compute AIC and KS test."""
    shape, loc, scale = lognorm.fit(data, floc=0)
    ll = np.sum(lognorm.logpdf(data, shape, loc=loc, scale=scale))
    aic = 2 * 2 - 2 * ll
    ks_stat, ks_p = kstest(data, 'lognorm', args=(shape, loc, scale))
    return ("Lognormal", shape, scale, aic, ks_stat, ks_p)


def fit_weibull_distribution(data: np.ndarray) -> tuple:
    """Fit Weibull distribution using MLE, compute AIC and KS test."""
    shape, loc, scale = weibull_min.fit(data, floc=0)
    ll = np.sum(weibull_min.logpdf(data, shape, loc=loc, scale=scale))
    aic = 2 * 2 - 2 * ll
    ks_stat, ks_p = kstest(data, 'weibull_min', args=(shape, loc, scale))
    return ("Weibull", shape, scale, aic, ks_stat, ks_p)


def fit_poisson_distribution(data: np.ndarray) -> tuple:
    """Fit Poisson distribution (approx) and compute AIC and KS test."""
    lam = np.mean(data)
    ll = np.sum(poisson.logpmf(np.round(data), lam))
    aic = 2 * 1 - 2 * ll
    ks_stat, ks_p = kstest(np.round(data), 'poisson', args=(lam,))
    return ("Poisson", lam, None, aic, ks_stat, ks_p)


def plot_distributions(data: np.ndarray, fits: list, column: str) -> None:
    """Plot histogram and fitted curves for all distributions."""
    plt.figure(figsize=(10, 6))
    plt.hist(data, bins=30, alpha=0.6, color="gray", density=True,
             label="Observed Data")
    x = np.linspace(data.min(), data.max(), 200)

    for dist_name, p1, p2, _, _, _ in fits:
        if dist_name == "Gamma":
            pdf = gamma.pdf(x, p1, scale=p2)
        elif dist_name == "Lognormal":
            pdf = lognorm.pdf(x, p1, scale=p2)
        elif dist_name == "Weibull":
            pdf = weibull_min.pdf(x, p1, scale=p2)
        elif dist_name == "Poisson":
            pdf = poisson.pmf(np.round(x), p1)
        else:
            continue
        plt.plot(x, pdf, label=f"{dist_name}")

    plt.title(f"Fitted Distributions for '{column}'")
    plt.xlabel(column)
    plt.ylabel("Density")
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()


def compare_distributions(fits: list) -> None:
    """Print parameters, AIC, KS and ranking."""
    print("=== Comparison of Distributions ===")
    for dist_name, p1, p2, aic, ks_stat, ks_p in fits:
        print(f"{dist_name}:")
        print(f"  Params: {p1:.4f}" + (f", {p2:.4f}" if p2 else ""))
        print(f"  AIC: {aic:.2f}")
        print(f"  KS stat: {ks_stat:.4f}, p-value: {ks_p:.4f}\n")

    # Ranking by AIC
    sorted_aic = sorted(fits, key=lambda x: x[3])
    print("Best fit by AIC:", sorted_aic[0][0])

    # Ranking by KS stat (lower is better)
    sorted_ks = sorted(fits, key=lambda x: x[4])
    print("Best fit by KS:", sorted_ks[0][0])
```

```{python}
target_col = "% Silica Concentrate"
target_data = load_and_validate_data(df_data, target_col)

fits = [
    fit_gamma_distribution(target_data),
    fit_lognormal_distribution(target_data)
]

plot_distributions(target_data, fits, target_col)
compare_distributions(fits)
```

Analisando as curvas em comparação ao histograma, o AIC e o teste de hipótese, a distribuição lognormal aparenta ter um melhor ajuste  que a Gamma.
Apesar disso, o teste KS retornou p-valor igual a 0 para ambas as distribuições, rejeitando a hipótese nula de que os dados pertencem a uma das duas distribuições. 

Nesse momento, não descartamos nenhuma das duas distribuições, apenas manteremos essas informações em mente ao longo das análises. 

